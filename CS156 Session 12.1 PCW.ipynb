{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS156 Session 12.1 PCW.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPNUGpfsmgOTQQYHfVNVfNW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEnJIOsSSxLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjTlk0jrSzvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsFiAwdKQt4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import *\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "import gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giSPT_XeQ2UW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "book = open(\"therepublic.txt\", \"r\")\n",
        "book = book.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8BxqkVlQ-r5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = PorterStemmer()\n",
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "# Tokenize and lemmatize\n",
        "\n",
        "def preprocess(text):\n",
        "    result=[]\n",
        "    for token in gensim.utils.simple_preprocess(text) :\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "            \n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4s96u5DS9h2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "0a3ff4c0-1485-4f37-c086-d1e57c334e40"
      },
      "source": [
        "'''\n",
        "Preview a document after preprocessing\n",
        "'''\n",
        "print(\"Original document: \")\n",
        "words = []\n",
        "for word in book.split(' '):\n",
        "    words.append(word)\n",
        "print(words[:20])\n",
        "print(\"\\n\\nTokenized and lemmatized document: \")\n",
        "print(preprocess(book)[:20])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original document: \n",
            "['\\ufeffThe', 'Project', 'Gutenberg', 'EBook', 'of', 'The', 'Republic,', 'by', 'Plato\\n\\nThis', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost']\n",
            "\n",
            "\n",
            "Tokenized and lemmatized document: \n",
            "['project', 'gutenberg', 'ebook', 'republ', 'plato', 'ebook', 'cost', 'restrict', 'whatsoev', 'copi', 'away', 'term', 'project', 'gutenberg', 'licens', 'includ', 'ebook', 'onlin', 'gutenberg', 'titl']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNjDxfQsQ_dU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "book = lemmatize_stemming(book)\n",
        "processed_book = preprocess(book)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fj_3EKeGTnD4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "05feea09-50b8-4e37-d3b7-665b6d9b0c6f"
      },
      "source": [
        "'''\n",
        "Preview 'processed_book'\n",
        "'''\n",
        "print(processed_book[:20])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['project', 'gutenberg', 'ebook', 'republ', 'plato', 'ebook', 'cost', 'restrict', 'whatsoev', 'copi', 'away', 'term', 'project', 'gutenberg', 'licens', 'includ', 'ebook', 'onlin', 'gutenberg', 'titl']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82HUHXCvRHIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Create a dictionary from 'processed_docs' containing the number of times a word appears \n",
        "in the training set using gensim.corpora.Dictionary and call it 'dictionary'\n",
        "'''\n",
        "dictionary = gensim.corpora.Dictionary([word.split() for word in processed_book])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZMI8c-lT47E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "a608759e-13a4-4644-c61e-7cf68b44b842"
      },
      "source": [
        "'''\n",
        "Checking dictionary created\n",
        "'''\n",
        "count = 0\n",
        "for k, v in dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 project\n",
            "1 gutenberg\n",
            "2 ebook\n",
            "3 republ\n",
            "4 plato\n",
            "5 cost\n",
            "6 restrict\n",
            "7 whatsoev\n",
            "8 copi\n",
            "9 away\n",
            "10 term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5IwxeyNUBeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "OPTIONAL STEP\n",
        "Remove very rare and very common words:\n",
        "\n",
        "- words appearing less than 15 times\n",
        "- words appearing in more than 10% of all documents\n",
        "'''\n",
        "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZaA1En6RiZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "'''\n",
        "Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
        "words and how many times those words appear. Save this to 'bow_corpus'\n",
        "'''\n",
        "bow_corpus = [dictionary.doc2bow(processed_book)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et4lo9AmSMUK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "7287f29c-fea7-4518-8e8a-17c243120bf8"
      },
      "source": [
        "'''\n",
        "Preview BOW for our sample preprocessed document\n",
        "'''\n",
        "for i in range(len(bow_corpus)):\n",
        "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_corpus[i][0], \n",
        "                                                     dictionary[bow_corpus[i][0]], \n",
        "                                                     bow_corpus[i][1]))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-9617ff0863c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_corpus[i][0], \n\u001b[0;32m----> 7\u001b[0;31m                                                      \u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbow_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                                                      bow_corpus[i][1]))\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, tokenid)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;31m# recompute id->word accordingly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrevdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenid\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# will throw for non-existent ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: (0, 88)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt7A2AxFUxp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LDA mono-core -- fallback code in case LdaMulticore throws an error on your machine\n",
        "# lda_model = gensim.models.LdaModel(bow_corpus, \n",
        "#                                    num_topics = 10, \n",
        "#                                    id2word = dictionary,                                    \n",
        "#                                    passes = 50)\n",
        "\n",
        "# LDA multicore \n",
        "'''\n",
        "Train your lda model using gensim.models.LdaMulticore and save it to 'lda_model'\n",
        "'''\n",
        "# TODO\n",
        "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
        "                                   num_topics = 10, \n",
        "                                   id2word = dictionary,                                    \n",
        "                                   passes = 10,\n",
        "                                   workers = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMD4ybvOVQUE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "outputId": "cb24f87f-5910-443b-e5e1-bf65415eadf5"
      },
      "source": [
        "'''\n",
        "For each topic, we will explore the words occuring in that topic and its relative weight\n",
        "'''\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.014*\"say\" + 0.009*\"state\" + 0.006*\"like\" + 0.005*\"good\" + 0.005*\"natur\" + 0.004*\"true\" + 0.004*\"think\" + 0.004*\"certainli\" + 0.004*\"time\" + 0.004*\"thing\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.027*\"say\" + 0.015*\"good\" + 0.012*\"true\" + 0.012*\"state\" + 0.010*\"like\" + 0.008*\"know\" + 0.007*\"thing\" + 0.007*\"life\" + 0.006*\"natur\" + 0.006*\"soul\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.010*\"say\" + 0.006*\"good\" + 0.005*\"like\" + 0.005*\"state\" + 0.005*\"natur\" + 0.005*\"true\" + 0.004*\"life\" + 0.004*\"repli\" + 0.004*\"think\" + 0.004*\"thing\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.009*\"say\" + 0.008*\"state\" + 0.008*\"good\" + 0.006*\"true\" + 0.006*\"like\" + 0.005*\"natur\" + 0.005*\"soul\" + 0.005*\"life\" + 0.004*\"plato\" + 0.004*\"think\"\n",
            "\n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.022*\"say\" + 0.014*\"state\" + 0.012*\"good\" + 0.010*\"like\" + 0.010*\"true\" + 0.009*\"natur\" + 0.007*\"life\" + 0.006*\"soul\" + 0.006*\"plato\" + 0.006*\"thing\"\n",
            "\n",
            "\n",
            "Topic: 5 \n",
            "Words: 0.010*\"say\" + 0.007*\"state\" + 0.006*\"like\" + 0.006*\"good\" + 0.005*\"true\" + 0.004*\"think\" + 0.004*\"natur\" + 0.004*\"mean\" + 0.003*\"thing\" + 0.003*\"evil\"\n",
            "\n",
            "\n",
            "Topic: 6 \n",
            "Words: 0.012*\"say\" + 0.006*\"state\" + 0.006*\"good\" + 0.006*\"true\" + 0.005*\"natur\" + 0.004*\"like\" + 0.004*\"justic\" + 0.004*\"life\" + 0.004*\"thing\" + 0.003*\"plato\"\n",
            "\n",
            "\n",
            "Topic: 7 \n",
            "Words: 0.024*\"say\" + 0.011*\"good\" + 0.010*\"state\" + 0.009*\"true\" + 0.008*\"life\" + 0.007*\"like\" + 0.007*\"natur\" + 0.006*\"soul\" + 0.006*\"justic\" + 0.005*\"pleasur\"\n",
            "\n",
            "\n",
            "Topic: 8 \n",
            "Words: 0.019*\"say\" + 0.011*\"like\" + 0.010*\"true\" + 0.010*\"good\" + 0.010*\"state\" + 0.006*\"natur\" + 0.006*\"justic\" + 0.006*\"thing\" + 0.006*\"time\" + 0.006*\"knowledg\"\n",
            "\n",
            "\n",
            "Topic: 9 \n",
            "Words: 0.018*\"say\" + 0.011*\"good\" + 0.009*\"true\" + 0.009*\"state\" + 0.008*\"natur\" + 0.007*\"like\" + 0.005*\"life\" + 0.005*\"justic\" + 0.004*\"plato\" + 0.004*\"thing\"\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}